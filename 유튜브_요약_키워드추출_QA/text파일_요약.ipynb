{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bece519e",
   "metadata": {},
   "source": [
    "# 합친것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a0281c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:29: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:29: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\82102\\AppData\\Local\\Temp/ipykernel_18324/4064985180.py:29: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if sentence is not '':\n"
     ]
    }
   ],
   "source": [
    "def summary(user_text):\n",
    "    yt = YouTube(user_text)\n",
    "    caption = yt.captions.get_by_language_code('ko.7XP2tGORuV4')\n",
    "    if caption == None:\n",
    "        caption = yt.captions.all()[0]\n",
    "    text = caption.xml_captions\n",
    "    불용어 = ['<[^가-힣]+>', '&[^가-힣]+;', '(박수 소리)', '\\n', '( )', ':' , '(앵커)', '(기자)']\n",
    "    for i in 불용어:\n",
    "        text = re.sub(i,' ', text)\n",
    "        \n",
    "    kkma = Kkma()\n",
    "\n",
    "    # text입력받아 kkma.sentences()를 이용해 문장단위로 나는 뒤 sentences로 리턴\n",
    "    def text2sentences(text):\n",
    "        sentences= kkma.sentences(text)  # text일 때 문장별로 리스트 만듦\n",
    "        for idx in range(0, len(sentences)): #길이에 따라 문장 합침\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "    sentences = text2sentences(text)\n",
    "    twitter = Twitter()\n",
    "\n",
    "    #불용어제거\n",
    "    stopwords = ['( )','안녕하세요', '아', '으', '네' ,'코']\n",
    "    def get_nouns(sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence is not '':\n",
    "                nouns.append(' '.join([noun for noun in twitter.nouns(str(sentence))\n",
    "                                    if noun not in stopwords and len(noun) > 1]))\n",
    "        return nouns\n",
    "    nouns = get_nouns(sentences)\n",
    "    tfidf = TfidfVectorizer()\n",
    "    cnt_vec = CountVectorizer()\n",
    "    graph_sentence = []\n",
    "    def build_sent_graph(sentence):\n",
    "        tfidf_mat = tfidf.fit_transform(sentence).toarray()\n",
    "        graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return graph_sentence\n",
    "\n",
    "    sent_graph = build_sent_graph(nouns)\n",
    "    def build_words_graph(sentence):\n",
    "        cnt_vec_mat = normalize(cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "    words_graph, idx2word = build_words_graph(nouns)\n",
    "    def get_ranks(graph, d=0.85):\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0\n",
    "            link_sum = np.sum(A[:,id])\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "        \n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B)\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "    sent_rank_idx = get_ranks(sent_graph)\n",
    "    sorted_sent_rank_idx = sorted(sent_rank_idx, key=lambda k: sent_rank_idx[k], reverse=True)\n",
    "    word_rank_idx = get_ranks(words_graph)\n",
    "    sorted_word_rank_idx = sorted(word_rank_idx, key=lambda k: word_rank_idx[k], reverse=True)\n",
    "    \n",
    "    summarize = []\n",
    "    index = []\n",
    "    for idx in sorted_sent_rank_idx[:5]:\n",
    "        index.append(idx)\n",
    "        \n",
    "    index.append(idx)\n",
    "    \n",
    "    for idx in index:\n",
    "        summarize.append(sentences[idx])\n",
    "        \n",
    "    for text in summarize[:]:\n",
    "        llist.append(text)\n",
    "    \n",
    "    m = list(set(llist))\n",
    "    \n",
    "    keywords = []\n",
    "    index = []\n",
    "    for idx in sorted_word_rank_idx[:15]:\n",
    "        index.append(idx)\n",
    "        \n",
    "    for idx in index:\n",
    "        keywords.append(idx2word[idx])\n",
    "        \n",
    "    keywords = [each_word for each_word in keywords\n",
    "                if each_word not in stop_words]\n",
    "        \n",
    "    keyword = keywords[:5]\n",
    "    return m, keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b8fa4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "import re\n",
    "user_text = 'https://www.youtube.com/watch?v=BNlZsg5RPxM&list=PLUHG6IBxDr3i8GaZ6wur2VGjr8HJjbGaz&index=46'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8d0e2434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp/ipykernel_18324/4064985180.py:3: DeprecationWarning: Call to deprecated function get_by_language_code (This object can be treated as a dictionary, i.e. captions['en']).\n",
      "  caption = yt.captions.get_by_language_code('ko.7XP2tGORuV4')\n",
      "C:\\Users\\82102\\anaconda3.1\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "n,key=summary(user_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9ad1a80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 성장이냐',\n",
       " '',\n",
       " '지금은 물가 잡는 게 가장 급하기 때문에 환영 받지 않더라도 금리 인상 신호를 주며 할 일을 하겠다는 겁니다.',\n",
       " '-( ) 새 한국은행 총재 인사청문회에서 이 창용 후보자가 금리를 올리겠다는 신호를 강하게 보냈습니다.',\n",
       " '새 한국은행 총재 인사청문회에서 이 창용 후보자가 금리를 올리겠다는 신호를 강하게 보냈습니다.',\n",
       " '악역도 마다하지 않겠다고 했습니다.',\n",
       " '미국처럼 물가가 오른 뒤가 아닌 선제적으로 금리를 올려야 한다고 강조했습니다.',\n",
       " '지금까지 사회부 신용 식 기자였습니다.',\n",
       " '는 질문에 이 창용 한국은행 총재 후보자는 지금은 물가부터 잡아야 할 때라는 답을 내놨습니다.']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "73ebca09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['물가', '금리', '정부', '정책', '인상']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179e8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1b023fe",
   "metadata": {},
   "source": [
    "# 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4823ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(user_text):\n",
    "    yt = YouTube(user_text)\n",
    "    caption = yt.captions.get_by_language_code('ko.7XP2tGORuV4')\n",
    "    if caption == None:\n",
    "        caption = yt.captions.all()[0]\n",
    "    text = caption.xml_captions\n",
    "    불용어 = ['<[^가-힣]+>', '&[^가-힣]+;', '(박수 소리)', '\\n', '( )', ':' , '(앵커)', '(기자)']\n",
    "    for i in 불용어:\n",
    "        text = re.sub(i,' ', text)\n",
    "        \n",
    "    kkma = Kkma()\n",
    "\n",
    "    # text입력받아 kkma.sentences()를 이용해 문장단위로 나는 뒤 sentences로 리턴\n",
    "    def text2sentences(text):\n",
    "        sentences= kkma.sentences(text)  # text일 때 문장별로 리스트 만듦\n",
    "        for idx in range(0, len(sentences)): #길이에 따라 문장 합침\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "    sentences = text2sentences(text)\n",
    "    twitter = Twitter()\n",
    "\n",
    "    #불용어제거\n",
    "    stopwords = ['( )','안녕하세요', '아', '으', '네' ,'코']\n",
    "    def get_nouns(sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence is not '':\n",
    "                nouns.append(' '.join([noun for noun in twitter.nouns(str(sentence))\n",
    "                                    if noun not in stopwords and len(noun) > 1]))\n",
    "        return nouns\n",
    "    nouns = get_nouns(sentences)\n",
    "    tfidf = TfidfVectorizer()\n",
    "    cnt_vec = CountVectorizer()\n",
    "    graph_sentence = []\n",
    "    def build_sent_graph(sentence):\n",
    "        tfidf_mat = tfidf.fit_transform(sentence).toarray()\n",
    "        graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return graph_sentence\n",
    "\n",
    "    sent_graph = build_sent_graph(nouns)\n",
    "    def build_words_graph(sentence):\n",
    "        cnt_vec_mat = normalize(cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "    words_graph, idx2word = build_words_graph(nouns)\n",
    "    def get_ranks(graph, d=0.85):\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0\n",
    "            link_sum = np.sum(A[:,id])\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "        \n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B)\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "    sent_rank_idx = get_ranks(sent_graph)\n",
    "    sorted_sent_rank_idx = sorted(sent_rank_idx, key=lambda k: sent_rank_idx[k], reverse=True)\n",
    "    word_rank_idx = get_ranks(words_graph)\n",
    "    sorted_word_rank_idx = sorted(word_rank_idx, key=lambda k: word_rank_idx[k], reverse=True)\n",
    "    \n",
    "    summarize = []\n",
    "    index = []\n",
    "    for idx in sorted_sent_rank_idx[:5]:\n",
    "        index.append(idx)\n",
    "        \n",
    "    index.append(idx)\n",
    "    \n",
    "    for idx in index:\n",
    "        summarize.append(sentences[idx])\n",
    "        \n",
    "    for text in summarize[:]:\n",
    "        llist.append(text)\n",
    "    \n",
    "    m = list(set(llist))\n",
    "    \n",
    "    keywords = []\n",
    "    index = []\n",
    "    for idx in sorted_word_rank_idx[:15]:\n",
    "        index.append(idx)\n",
    "        \n",
    "    for idx in index:\n",
    "        keywords.append(idx2word[idx])\n",
    "        \n",
    "    keywords = [each_word for each_word in keywords\n",
    "                if each_word not in stop_words]\n",
    "        \n",
    "    keyword = keywords[:5]\n",
    "    return m, keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4d57fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['구매 개수를 제한할 정도로 지금 그야말로 식용유 대란입니다. 치킨은 뭘로 튀기나 자영업자 분들의 하소연도 이어지고 있는데요. 어려운 사정은 국내뿐만이 아닙니다. 지금 전 세계가 그야말로 식용유 보릿고개를 넘고 있습니다. 앞서 스페인 벨기에 등 유럽 각국의 대형마트에서도 식용유의 구매 개수를 제한한 바 있는데요. 특히 영국의 대표적인 튀김 체인점인 피시인 칩스가 3천 곳 넘게 문을 닫았고 영국 정부까지 나서 체인점 폐점 위기를 경고할 정도입니다 그렇다면 전 세계 식용유 대란의 이유는 무엇일까요? 세계적으로 가장 많이 쓰는 3대 식용유는 카놀라유 콩기름 팜류입니다. 그중에 카놀라유 최대 수출국인 캐나다와 콩기름 원료 대두의 주산지인 남미에서 지난해부터 극심한 가뭄이 이어진 탓에 작황이 부진했죠. 보통 이런 상황에서는 해바라기 시유가 대안이 되곤 하죠. 그런데 해바라기 시유 수출 1 2위 국가가 우크라이나와 러시아입니다. 두 나라는 현재 전쟁 중이죠 우크라이나는 해바라기 CU 생산량의 절반 가량을 차지하고 있기 때문에 부족 현상은 더욱 악화됐습니다. 전쟁의 여파는 남은 배 안인 황류에도 영향을 미쳤습니다. 세계 최대 판류 생산국인 인도네시아가 지난달 28일 합류 수출을 막아버렸기 때문입니다. 전쟁으로 식용유 가격이 치솟자 인도네시아 판류 업자들이 환율을 더 비싸게 팔기 위해 생산량 대부분을 수출하면서 국내분이 부족해졌고 결국 인도네시아 정부가 나서 수출 금지 조치를 단행합니다. 결국 우리나라도 우크라이나 전쟁의 여진을 겪는 것이나 마찬가지입니다. 국내 식용유 가격이 오르고 구매 개수까지 제한하자 우려의 목소리는 더욱 커지고 있는데요. 다만 국내에서는 말레이시아산 판류를 주로 사용하고 아직 가지 원재료 수급에도 큰 문제가 없다는 게 전문가 의견입니다. 게다가 인도네시아의 수출 제한 조치도 완화될 가능성이 나오는 만큼 주의를 조금 더 지켜봐야 할 것 같습니다 ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('C:/Users/82102/Desktop/4월 프로젝트/total_texts.txt', 'r', encoding='UTF8') as file: \n",
    "    a = file.readlines()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd8dbeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국은행은 오늘 금융통화위원회를 열고 이번 달 기준금리를 결정합니다. 현재 1.25%인 기준금리를 0.25%포인트 올릴 것이라는 전망과 동결할 것이라는 전망이 엇갈리고 있습니다. 최근 4%대의 가파른 물가 상승률과 미국 연방준비제도의 예상보다 빠른 긴축 가능성 때문에 인상할 것이라는 전망이 나옵니다. 다만 한국은행 총재가 공석인 데다 경기 침체나 이자 부담이 늘어날 우려에 동결 분위기도 감지됩니다.  '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a[0].replace('니다', '니다. ')\n",
    "b = b.replace('요 ', ' 요. ')\n",
    "b = b.replace('. .', '.')\n",
    "b = b.replace('-(앵커)', '')\n",
    "b = b.replace('-(기자)', '')\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8573b6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한국은행은 오늘 금융통화위원회를 열고 이번 달 기준금리를 결정합니다',\n",
       " ' 현재 1',\n",
       " '25%인 기준금리를 0',\n",
       " '25%포인트 올릴 것이라는 전망과 동결할 것이라는 전망이 엇갈리고 있습니다',\n",
       " ' 최근 4%대의 가파른 물가 상승률과 미국 연방준비제도의 예상보다 빠른 긴축 가능성 때문에 인상할 것이라는 전망이 나옵니다',\n",
       " ' 다만 한국은행 총재가 공석인 데다 경기 침체나 이자 부담이 늘어날 우려에 동결 분위기도 감지됩니다',\n",
       " '  ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = b.split('.')\n",
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9d897d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국은행은 오늘 금융통화위원회를 열고 이번 달 기준금리를 결정합니다. 현재 1.25%인 기준금리를 0.25%포인트 올릴 것이라는 전망과 동결할 것이라는 전망이 엇갈리고 있습니다. 최근 4%대의 가파른 물가 상승률과 미국 연방준비제도의 예상보다 빠른 긴축 가능성 때문에 인상할 것이라는 전망이 나옵니다. 다만 한국은행 총재가 공석인 데다 경기 침체나 이자 부담이 늘어날 우려에 동결 분위기도 감지됩니다.  '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = b\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcee694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Twitter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a6cddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()\n",
    "\n",
    "# text입력받아 kkma.sentences()를 이용해 문장단위로 나는 뒤 sentences로 리턴\n",
    "def text2sentences(text):\n",
    "    sentences= kkma.sentences(text)  # text일 때 문장별로 리스트 만듦\n",
    "    for idx in range(0, len(sentences)): #길이에 따라 문장 합침\n",
    "        if len(sentences[idx]) <= 10:\n",
    "            sentences[idx-1] += (' ' + sentences[idx])\n",
    "            sentences[idx] = ''\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49c0af37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한국은행은 오늘 금융통화위원회를 열고 이번 달 기준금리를 결정합니다.',\n",
       " '현재 1.25% 인 기준금리를 0.25% 포인트 올릴 것이라는 전망과 동결할 것이라는 전망이 엇갈리고 있습니다.',\n",
       " '최근 4% 대의 가파른 물가 상승률과 미국 연방준비제도의 예상보다 빠른 긴축 가능성 때문에 인상할 것이라는 전망이 나옵니다.',\n",
       " '다만 한국은행 총재가 공석인 데다',\n",
       " '경 기 침체나 이자 부담이 늘어날 우려에 동결 분위기도 감지됩니다.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#기사본문 가져오기\n",
    "sentences = text2sentences(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "462b47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twitter()\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "#불용어제거\n",
    "stopwords = ['( )','안녕하세요', '아', '으', '네' ,'코']\n",
    "def get_nouns(sentences):\n",
    "    nouns = []\n",
    "    for sentence in sentences:\n",
    "        if sentence != '':\n",
    "            nouns.append(' '.join([noun for noun in twitter.nouns(str(sentence))\n",
    "                                  if noun not in stopwords and len(noun) > 1]))\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3b11244",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'twitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11484/2145731357.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnouns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_nouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11484/3580501827.py\u001b[0m in \u001b[0;36mget_nouns\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             nouns.append(' '.join([noun for noun in twitter.nouns(str(sentence))\n\u001b[0m\u001b[0;32m     11\u001b[0m                                   if noun not in stopwords and len(noun) > 1]))\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnouns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'twitter' is not defined"
     ]
    }
   ],
   "source": [
    "nouns = get_nouns(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "900dcbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "지금 회부 신용 기자\n"
     ]
    }
   ],
   "source": [
    "nouns = get_nouns(sentences)\n",
    "print(sentences[0])\n",
    "print(nouns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e402f67",
   "metadata": {},
   "source": [
    "# TF-IDF 모델 생성 및 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6d8f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "cnt_vec = CountVectorizer()\n",
    "graph_sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "42f621ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.1368084 , 0.2344078 , 0.        ,\n",
       "        0.14570008, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.21462631, 0.        , 0.        ,\n",
       "        0.59807016, 0.08045192, 0.        , 0.07007087, 0.07795253,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.1368084 , 0.21462631, 1.        , 0.06879399, 0.        ,\n",
       "        0.16783317, 0.13297177, 0.        , 0.24369379, 0.07522276,\n",
       "        0.13318705, 0.        , 0.038088  ],\n",
       "       [0.2344078 , 0.        , 0.06879399, 1.        , 0.        ,\n",
       "        0.07326516, 0.07455684, 0.        , 0.06493646, 0.        ,\n",
       "        0.04911892, 0.        , 0.05131671],\n",
       "       [0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.34743233, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.14570008, 0.59807016, 0.16783317, 0.07326516, 0.        ,\n",
       "        1.        , 0.05893372, 0.        , 0.05132926, 0.        ,\n",
       "        0.03882622, 0.        , 0.04056347],\n",
       "       [0.        , 0.08045192, 0.13297177, 0.07455684, 0.        ,\n",
       "        0.05893372, 1.        , 0.        , 0.12551557, 0.08152415,\n",
       "        0.03951073, 0.        , 0.04127862],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.07007087, 0.24369379, 0.06493646, 0.34743233,\n",
       "        0.05132926, 0.12551557, 0.        , 1.        , 0.07100475,\n",
       "        0.0344125 , 0.        , 0.03595227],\n",
       "       [0.        , 0.07795253, 0.07522276, 0.        , 0.        ,\n",
       "        0.        , 0.08152415, 0.        , 0.07100475, 1.        ,\n",
       "        0.        , 0.19696656, 0.08344776],\n",
       "       [0.        , 0.        , 0.13318705, 0.04911892, 0.        ,\n",
       "        0.03882622, 0.03951073, 0.        , 0.0344125 , 0.        ,\n",
       "        1.        , 0.08515665, 0.02719483],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.19696656,\n",
       "        0.08515665, 1.        , 0.13991683],\n",
       "       [0.        , 0.        , 0.038088  , 0.05131671, 0.        ,\n",
       "        0.04056347, 0.04127862, 0.        , 0.03595227, 0.08344776,\n",
       "        0.02719483, 0.13991683, 1.        ]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_sent_graph(sentence):\n",
    "    tfidf_mat = tfidf.fit_transform(sentence).toarray()\n",
    "    graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "    return graph_sentence\n",
    "\n",
    "sent_graph = build_sent_graph(nouns)\n",
    "sent_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b9d4cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words_graph(sentence):\n",
    "    cnt_vec_mat = normalize(cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "    vocab = cnt_vec.vocabulary_\n",
    "    return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4228b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_graph, idx2word = build_words_graph(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dd7c53d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "12279119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{47: '지금',\n",
       " 56: '회부',\n",
       " 29: '신용',\n",
       " 11: '기자',\n",
       " 53: '한국은행',\n",
       " 51: '총재',\n",
       " 37: '인사청문회',\n",
       " 50: '창용',\n",
       " 57: '후보자',\n",
       " 10: '금리',\n",
       " 30: '신호',\n",
       " 18: '물가',\n",
       " 1: '가장',\n",
       " 15: '때문',\n",
       " 55: '환영',\n",
       " 38: '인상',\n",
       " 12: '기호',\n",
       " 21: '보도',\n",
       " 27: '성장',\n",
       " 48: '질문',\n",
       " 19: '미국',\n",
       " 34: '오른',\n",
       " 25: '선제',\n",
       " 2: '강조',\n",
       " 31: '악역',\n",
       " 16: '마다',\n",
       " 32: '얼마나',\n",
       " 20: '보고',\n",
       " 4: '경제',\n",
       " 3: '결정',\n",
       " 24: '서민이',\n",
       " 17: '문제',\n",
       " 41: '정부',\n",
       " 49: '차원',\n",
       " 54: '해법',\n",
       " 13: '대출',\n",
       " 42: '정책',\n",
       " 23: '부동산',\n",
       " 7: '관련',\n",
       " 9: '규제',\n",
       " 35: '완화',\n",
       " 22: '부담',\n",
       " 46: '주기',\n",
       " 40: '점진',\n",
       " 0: '가야',\n",
       " 6: '과정',\n",
       " 28: '소통',\n",
       " 44: '조율',\n",
       " 39: '입장',\n",
       " 36: '윤석',\n",
       " 5: '공약',\n",
       " 52: '추경',\n",
       " 14: '대해',\n",
       " 26: '선택',\n",
       " 8: '규모',\n",
       " 33: '영향',\n",
       " 45: '조절',\n",
       " 43: '제언'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e63d0f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "for key, value in idx2word.items():\n",
    "    if value == '때문':\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d0294fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(graph, d=0.85):\n",
    "    A = graph\n",
    "    matrix_size = A.shape[0]\n",
    "    for id in range(matrix_size):\n",
    "        A[id, id] = 0\n",
    "        link_sum = np.sum(A[:,id])\n",
    "        if link_sum != 0:\n",
    "            A[:, id] /= link_sum\n",
    "        A[:, id] *= -d\n",
    "        A[id, id] = 1\n",
    "        \n",
    "    B = (1-d) * np.ones((matrix_size, 1))\n",
    "    ranks = np.linalg.solve(A, B)\n",
    "    return {idx: r[0] for idx, r in enumerate(ranks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0dfa9589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.752643164630335,\n",
       " 1: 1.3207022165881923,\n",
       " 2: 1.5994902489122675,\n",
       " 3: 0.9014321759198791,\n",
       " 4: 0.5610996455548302,\n",
       " 5: 1.4955609241323955,\n",
       " 6: 0.9078667575803402,\n",
       " 7: 0.15000000000000002,\n",
       " 8: 1.453794768405428,\n",
       " 9: 0.9001172902022545,\n",
       " 10: 0.651406370244879,\n",
       " 11: 0.7152666049584347,\n",
       " 12: 0.7406198328707547}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_rank_idx = get_ranks(sent_graph)\n",
    "sent_rank_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7536a6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 8, 1, 6, 3, 9, 0, 12, 11, 10, 4, 7]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_sent_rank_idx = sorted(sent_rank_idx, key=lambda k: sent_rank_idx[k], reverse=True)\n",
    "sorted_sent_rank_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e9639afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_rank_idx = get_ranks(words_graph)\n",
    "sorted_word_rank_idx = sorted(word_rank_idx, key=lambda k: word_rank_idx[k], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db85ae",
   "metadata": {},
   "source": [
    "# 요약하기 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1e59e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "llist=[]\n",
    "summary = []\n",
    "summarize=[]\n",
    "index = []\n",
    "for idx in sorted_sent_rank_idx[:5]:\n",
    "    index.append(idx)\n",
    "        \n",
    "index.append(idx)\n",
    "    \n",
    "for idx in index:\n",
    "    summary.append(sentences[idx])\n",
    "        \n",
    "for text in summary[:]:\n",
    "    llist.append(text)\n",
    "    \n",
    "m = list(set(llist))\n",
    "\n",
    "for i in m:\n",
    "    i = i.replace('-( )', '')\n",
    "    summarize.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "059f02cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 성장이냐  새 한국은행 총재 인사청문회에서 이 창용 후보자가 금리를 올리겠다는 신호를 강하게 보냈습니다. 미국처럼 물가가 오른 뒤가 아닌 선제적으로 금리를 올려야 한다고 강조했습니다. 지금까지 사회부 신용 식 기자였습니다.'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "09446984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  알겠습니다. 지금까지 사회부 신용식 기자였습니다.  새 한국은행 총재 인사청문회에서 이창용 후보자가 금리를 올리겠다는 신호를 강하게 보냈습니다. 지금은 물가 잡는 게 가장 급하기 때문에 환영받지 않더라도 금리 인상 신호를 주며 할 일을 하겠다는 겁니다. 조기호 기자의 보도입니다.  물가냐 성장이냐는 질문에 이창용 한국은행 총재 후보자는 지금은 물가부터 잡아야 할 때라는 답을 내놨습니다.  미국처럼 물가가 오른 뒤가 아닌 선제적으로 금리를 올려야 한다고 강조했습니다. 악역도 마다하지 않겠다고 했습니다.  금리 인상폭은 물가가 얼마나 오를지 보고 또 경제 성장에 해가 되지 않는 선에서 결정하겠다고 덧붙였습니다. 금리를 올렸을 때 서민이 힘들어지는 문제는 범정부 차원에서 해법을 내놔야 한다고 밝혔습니다. 또 대출 정책은 부동산과 관련돼 있고 규제를 완화하면 물가에 부담을 주기 때문에 점진적으로 가야 한다고도 했습니다. 이 과정에서 정부와 소통하고 조율을 하겠지만 정부 정책에 끌려가지는 않을 것이라는 입장도 내놨습니다.  윤석열 정부의 공약인 50조 원 추경에 대해서는 불가피한 선택이라면서도 규모는 물가에 미치는 영향을 따져서 조절해야 한다고 제언했습니다.  '"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = ' '.join(summarize)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f346c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['때문', '지금', '이', '을', '의', '는', '가', '들', ',', '도',\n",
    "              '은', '그', '.', '를', '너무', '고', '것' ,'로', '하는','보고','에','때','에',\n",
    "              '저','하고','에서','정말','못','다','까지','또','개','수','으로','만','할',\n",
    "              '과','인','님','-','임','7','봐도','하는거','그냥','해도','바로','순간',\n",
    "              '잘','한','겁나','나','같이','함께','라고','쓴','아니라','아닌','옹','살','진짜','하는게','같아요','제목','적', '가야','지금',\n",
    "             '관련']\n",
    "    \n",
    "keywords = []\n",
    "index = []\n",
    "for idx in sorted_word_rank_idx[:15]:\n",
    "    index.append(idx)\n",
    "        \n",
    "for idx in index:\n",
    "    keywords.append(idx2word[idx])\n",
    "        \n",
    "keywords = [each_word for each_word in keywords\n",
    "            if each_word not in stop_words]\n",
    "        \n",
    "keyword = keywords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "67574145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['물가', '금리', '정부', '정책', '인상']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d810c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f2b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0c1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3d925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13721c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cea859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ff12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70ab48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7395e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1eefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea879a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe9a742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce3524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8bcde3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f936177e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0fb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61fa69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8fc88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6479f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4992a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6d1d7c6",
   "metadata": {},
   "source": [
    "def summarize(sent_num=5):\n",
    "    summary = []\n",
    "    index = []\n",
    "    for idx in sorted_sent_rank_idx[:sent_num]:\n",
    "        index.append(idx)\n",
    "        \n",
    "    index.append(idx)\n",
    "    \n",
    "    for idx in index:\n",
    "        summary.append(sentences[idx])\n",
    "        \n",
    "    for text in summary[:]:\n",
    "        print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
